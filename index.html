<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Kuan-Lin (Tony) Chu</title>
    <link rel="stylesheet" href="style.css">
</head>
<body>
    <div class="container">
        <header>
            <div class="top-links">
                <a href="mailto:klchu1027@cs.nycu.edu.tw" target="_blank">
                    <img src="figs/email.png" alt="Email" class="icon"> klchu1027@cs.nycu.edu.tw
                </a>
                <a href="https://www.linkedin.com/in/kuan-lin-tony-chu-28a14b2a6/" target="_blank">
                    <img src="figs/linkedin.png" alt="LinkedIn" class="icon"> Kuan-Lin Chu
                </a>
                <a href="https://github.com/tonychu27" target="_blank">
                    <img src="figs/github.svg" alt="GitHub" class="icon"> tonychu27
                </a>
            </div>
            <h1>Kuan-Lin (Tony) Chu</h1>
            <img src="figs/tony.jpg" alt="Kuan-Lin Chu" class="profile-photo">
            <p>
                I recently received my B.S. degree in 
                <a href="https://www.cs.nycu.edu.tw/?locale=en" target="_blank">Computer Science</a> 
                from 
                <a href="https://www.nycu.edu.tw/nycu/en/" target="_blank">National Yang Ming Chiao Tung University (NYCU)</a>.
            </p>
            <p>My research focuses on large language model (LLM) safety, optimization, and efficient model compression. I am particularly interested in understanding the internal mechanisms of LLMs to improve their reliability, robustness, and efficiency in real-world applications.</p>
            <p>I have experience in both research and industry settings, working on projects that involve structured pruning, model quantization, and safety interventions for LLMs.</p>
            <p>My goal is to contribute to the development of trustworthy AI systems that are both high-performing and safe for deployment.</p>
            <p>If you are interested in collaborating or working with me, feel free to <a href="mailto:klchu1027@cs.nycu.edu.tw">contact me</a>.</p>
        </header>

        <hr>

        <section id="education">
        <h2>Education</h2>
        <div>
            <h3>B.S. in Computer Science, National Yang Ming Chiao Tung University (NYCU), Hsinchu, Taiwan, 2025</h3>
            <p><strong>Overall GPA:</strong> 4.11 / 4.3</p>
            <p><strong>Department:</strong> Department of Computer Science</p>
        </div>
        </section>

        <hr>

        <section id="research-experience">
        <h2>Research Experience</h2>

        <div>
            <h3>University of California, San Diego (UCSD), San Diego, CA</h3>
            <p><strong>Research Assistant</strong> (Advised by <a href="https://lilywenglab.github.io" target="_blank">Prof. Lily Weng</a>), Jun. 2025 – Sep. 2025</p>
            <p>Investigated internal attention mechanisms in large language models, focusing on detection heads that identify harmful content and refusal heads that drive the model to reject unsafe requests. Developed systematic methods to identify and analyze these key components.</p>
            <p>Proposed Detection Refusal Advanced LLM (DRefA) by amplifying detection and refusal heads, significantly improving model safety under adversarial attacks while minimally impacting general performance, contributing to safer and more reliable AI systems. With DRefA, the LLaMA3 safety rate improved from <strong>15% to 99%</strong> under ADV-LLM attacks.</p>
        </div>

        <div>
            <h3>AI System Testing Lab, National Yang Ming Chiao Tung University (NYCU), Hsinchu, Taiwan</h3>
            <p><strong>Research Assistant</strong> (Advised by <a href="https://people.cs.nycu.edu.tw/~tfchen/" target="_blank">Prof. Tien-Fu Chen</a>) Sep 2024 – Jun 2025</p>
            <p>Developed a novel pruning approach based on activation importance estimation using calibration data, applied to LLaMA-3.2-3B-Instruct with a 5% pruning ratio.</p>
            <p>Achieved a perplexity of <strong>12.23</strong>, reducing degradation by <strong>56%</strong> compared to random pruning (13.72), demonstrating higher efficiency with minimal loss in output quality.</p>
        </div>
        </section>

        <hr>

        <section id="work-experience">
        <h2>Work Experience</h2>
        <div>
            <h3>Micro-Star International, New Taipei City, Taiwan</h3>
            <p><strong>Research and Development Intern</strong>, Jul. 2024 – Jun. 2025</p>
            <p>Designed and implemented an advanced floor segmentation model for Autonomous Mobile Robots (AMRs) using camera-based perception, achieving <strong>90% segmentation accuracy</strong> on test environments.</p>
            <p>Deployed trained models to embedded systems using pruning and quantization, enabling <strong>real-time inference at 30+ FPS</strong> on resource-constrained hardware with <strong>~15% model size reduction</strong>.</p>
        </div>
        </section>

        <hr>

        <section id="publications">
            <h2>Publications</h2>
            <div>
                <p>
                    <strong>Kuan-Lin Chu</strong>, Chung-En Sun, and Tsui-Wei Weng. “How to Make LLMs Safer? Detecting and Editing Key Heads in LLMs.” <em>Preprint (accepted to NeurIPS 2025 Lock-LLM Workshop)</em>, 2025.
                </p>
            </div>
        </section>

        <hr>

        </div>
</body>
</html>