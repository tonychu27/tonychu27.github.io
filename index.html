<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Kuan-Lin (Tony) Chu</title>
    <link rel="stylesheet" href="style.css">
</head>
<body>
    <div class="container">
        <header>
            <h1>Kuan-Lin (Tony) Chu</h1>
            <img src="figs/tony.jpg" alt="Kuan-Lin Chu" class="profile-photo">
            <p>
            I recently received my B.S. degree in 
            <a href="https://cs.nycu.edu.tw/en/" target="_blank">Computer Science</a> 
            from 
            <a href="https://www.nycu.edu.tw/nycu/en/" target="_blank">National Yang Ming Chiao Tung University (NYCU)</a>.
            </p>
            <p>My research focuses on large language model (LLM) safety, optimization, and efficient model compression.</p>
        </header>

        <hr>

        <section id="education">
        <h2>Education</h2>
        <div>
            <h3>B.S. in Computer Science, National Yang Ming Chiao Tung University (NYCU), Hsinchu, Taiwan, 2025</h3>
            <p><strong>Overall GPA:</strong> 4.11 / 4.3</p>
            <p><strong>Department:</strong> Department of Computer Science</p>
        </div>
        </section>

        <hr>

        <section id="research-experience">
        <h2>Research Experience</h2>

        <div>
            <h3>University of California, San Diego (UCSD), San Diego, CA</h3>
            <p><strong>Research Assistant</strong> (Advised by Prof. Lily Weng), Jun. 2025 – Sep. 2025</p>
            <p>Investigated internal attention mechanisms in large language models, focusing on detection heads that identify harmful content and refusal heads that drive the model to reject unsafe requests. Developed systematic methods to identify and analyze these key components.</p>
            <p>Proposed Detection Refusal Advanced LLM (DRefA) by amplifying detection and refusal heads, significantly improving model safety under adversarial attacks while minimally impacting general performance, contributing to safer and more reliable AI systems. With DRefA, the LLaMA3 safety rate improved from <strong>15% to 99%</strong> under ADV-LLM attacks.</p>
        </div>

        <div>
            <h3>AI System Testing Lab, National Yang Ming Chiao Tung University (NYCU), Hsinchu, Taiwan</h3>
            <p><strong>Research Assistant</strong> (Advised by Prof. Tien-Fu Chen), Sep 2024 – Jun 2025</p>
            <p>Developed a novel pruning approach based on activation importance estimation using calibration data, applied to LLaMA-3.2-3B-Instruct with a 5% pruning ratio.</p>
            <p>Achieved a perplexity of <strong>12.23</strong>, reducing degradation by <strong>56%</strong> compared to random pruning (13.72), demonstrating higher efficiency with minimal loss in output quality.</p>
        </div>
        </section>

        <hr>

        <section id="work-experience">
        <h2>Work Experience</h2>

        <div>
            <h3>Micro-Star International, New Taipei, Taiwan</h3>
            <p><strong>Research and Development Intern</strong>, Jul. 2024 – Jun. 2025</p>
            <p>Designed and implemented an advanced floor segmentation model for Autonomous Mobile Robots (AMRs) using camera-based perception.</p>
            <p>Deployed trained models to embedded systems using pruning and quantization for efficient, real-time inference on resource-constrained hardware.</p>
        </div>
        </section>

        <hr>

        <section id="publications">
        <h2>Publications</h2>

        <div>
            <p><strong>Kuan-Lin, Chu</strong> and Chung-En, Sun and Tsui-Wei, Weng, “How to Make LLMs Safer? Detecting and Editing Key Heads in LLMs”, NeurIPS 2025 Lock-LLM Workshop</p>
        </div>
        </section>

        <hr>

        </div>
</body>
</html>